Agentic Interview Preparation & Resume Stress-Testing
Assistant
48-Hour Hackathon Problem Statement
Background
Candidates often prepare for interviews using generic question lists and random mock interviews, without
understanding which parts of their resume are likely to be challenged. This problem focuses on building an
intelligent agent that prepares candidates for real interview scrutiny in a privacy-safe manner.
Problem Statement
Build an agentic AI system that analyzes a candidate’s resume and self-declared background to generate realistic,
resume-driven interview questions, evaluate resume interview-readiness, and provide actionable preparation
guidance. The system must not use any proprietary or InterviewBuddy internal data.
Core Inputs
• Resume (PDF or text)
• Candidate profile (role, experience, tech stack, strengths, weaknesses)
• Optional public job description
Functional Requirements
1. Resume parsing and claim extraction
2. Identification of weak, ambiguous, or high-risk resume claims
3. Resume-driven interview question generation
4. Interview-readiness scoring
5. Personalized preparation and coaching insights
Agentic Architecture Requirements
The system must be implemented using a multi-agent architecture:
• Resume Analyst Agent
• Question Strategist Agent
• Difficulty Planner Agent
• Validator Agent
• Coach Agent
Agents must communicate via structured context and be orchestrated using LangGraph.
Technology Constraints (Mandatory)
• Python system utilities (logger, retry, config loader)
• REST APIs for resume upload and analysis
• ML models from scratch (minimum 2)
• Feature engineering (30+ features)
• NLP pipeline (tokenization, embeddings, clarity scoring)
• Neural network from scratch
• LLM usage with retries and cost controls
• LangChain and LangGraph for agent orchestration
Outputs
• Interview-readiness score
• Resume vulnerability map
• Curated interview question set
• Preparation recommendations
• Explanation of reasoning behind scores and questions
Restrictions
• No InterviewBuddy internal data
• No black-box ML models for scoring
• No single-prompt or non-agentic solutions
Difficulty Level
Medium to Hard – emphasis on system design, explainability, and correctness.
Evaluation Focus
• System design quality
• Correct use of mandated technologies
• Clarity and explainability
• Agent orchestration logic
• Practical usefulness of outputs